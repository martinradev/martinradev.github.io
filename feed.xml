<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Martin&#39;s blog</title>
    <description></description>
    <link>http://yourdomain.com/</link>
    <atom:link href="http://yourdomain.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 30 Dec 2019 12:53:39 +0100</pubDate>
    <lastBuildDate>Mon, 30 Dec 2019 12:53:39 +0100</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>Write-up for hxp&#39;s tetres2019 challenge</title>
        <description>&lt;h3 id=&quot;the-game&quot;&gt;The game&lt;/h3&gt;
&lt;p&gt;The game is tetris where, according to the rules, one has to score a lot of points to receive the flag.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/game.png&quot; alt=&quot;Tetris game&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The game doesn’t really give good figures to be able to score easily, the target score is not disclosed and since this is a reverse engineering challenge, it’s probably be best to go in the direction of reversing the game.&lt;/p&gt;

&lt;p&gt;The challenge explicitly mentions that OpenGL 4.6 is required and that it only works on NVIDIA and Intel GPUs, so it would be best to look at the game in a graphics debugger.&lt;/p&gt;

&lt;h3 id=&quot;graphics-debugger&quot;&gt;Graphics debugger&lt;/h3&gt;
&lt;p&gt;I used RenderDoc as a graphics debugger to see what OpenGL calls are made and what the main rendering loop looks like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/gl_calls.jpg&quot; alt=&quot;GL calls&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that there are only two GL calls which make something useful: glDispatchCompute and glDrawArrays.
glDispatchCompute schedules a compute program to be executed and glDrawArrays schedules some primitives to be rendered.&lt;/p&gt;

&lt;p&gt;We can also view resources in the game like textures, shaders and the whole GL state.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/gl_resources.jpg&quot; alt=&quot;Resources&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By examining the compute shader, we can deduce that all of the game logic happens in it.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;layout(set = 0, binding = 0, std430) coherent buffer State
{
    int arr[512];
} _35;

layout(set = 0, binding = 1, std430) coherent buffer State2
{
    int arr2[512];
} _234;

layout(set = 0, binding = 3, std430) buffer MyBlock
{
    int x;
    int y;
    int tetris;
    int dirx;
    int diry;
    int tock;
    int mhm;
    int idid;
} _279;

layout(set = 0, binding = 2, std430) coherent buffer Meta
{
    int gen;
    int abra;
    int chch;
    int chacha;
    uint d[32];
} _299;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;_35&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;_234&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;_279&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;_299&lt;/code&gt; buffers contain state information and are used throughout the whole program.
In particular, the &lt;code class=&quot;highlighter-rouge&quot;&gt;Meta::d[32]&lt;/code&gt; buffer is used in the following snippet where it looks like something is being decoded:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    if ((uint(lowest) == row) &amp;amp;&amp;amp; (row != 0u))
    {
        uint q = _299.d[(col + uint(_299.abra)) % 32u];
        q ^= (uint(_299.abra) + col);
        _299.d[(col + uint(_299.abra)) % 32u] = (q &amp;lt;&amp;lt; uint(25)) | (q &amp;gt;&amp;gt; uint(10));
        q = _299.d[((col + uint(_299.abra)) + 16u) % 32u];
        q ^= ((uint(_299.abra) + col) + 16u);
        _299.d[((col + uint(_299.abra)) + 16u) % 32u] = (q &amp;lt;&amp;lt; uint(25)) | (q &amp;gt;&amp;gt; uint(10));
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And the &lt;code class=&quot;highlighter-rouge&quot;&gt;_299.abra&lt;/code&gt; variable keeps the score and is being incremented in a previous location in code:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if ((uint(lowest) == row) &amp;amp;&amp;amp; (row != 0u))
{
        if ((col != 0u) &amp;amp;&amp;amp; (col != 15u))
        {
            for (int r = int(row); r &amp;lt; 32; r++)
            {
                _234.arr2[(r * 16) + int(col)] = _35.arr[((r + 1) * 16) + int(col)];
                _35.arr[(r * 16) + int(col)] = _35.arr[((r + 1) * 16) + int(col)];
            }
        }
        if (col == 0u)
        {
            _299.abra++;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can also see what GL calls were used to create the compute shader which will become important later:
&lt;img src=&quot;/assets/gl_specialize_shader.jpg&quot; alt=&quot;SPIR-V calls&quot; class=&quot;center-image&quot; /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;glShaderBinary&lt;/code&gt; loads a binary file in a particular format and in this case it is in the SPIR-V format which was originally created to be used for the Vulkan API.
&lt;code class=&quot;highlighter-rouge&quot;&gt;glSpecializeShader&lt;/code&gt; is used to change constants and specify an entry point. For this shader it only specifies the &lt;code class=&quot;highlighter-rouge&quot;&gt;main()&lt;/code&gt; entry point.&lt;/p&gt;

&lt;p&gt;Other than that there isn’t much else interesting in the compute shader. Everything else is logic for moving blocks and collision detection.&lt;/p&gt;

&lt;p&gt;Next we examine the vertex and fragment shader used for rendering.
The vertex shader doesn’t do much but generate four vertices to create a rectangle to render to. This is a standard hacky way for creating a surface to render onto.
The fragment shader is fairly large but contains a lot of repetitive code.
The same GPU buffers persist in the fragment shader which means that they are shared with the compute shader.
There are a lot of arrays with indices which are used to compute coordinates in a texture.&lt;/p&gt;

&lt;p&gt;The arrays:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;int msg[5] = { 86, 56, 71, 37, 51 };
int msg_hoho[5] = {
	get_num(s, 10000), get_num(s, 1000), get_num(s, 100), get_num(s, 10), get_num(s, 1) };
int over[9] = { 93, 83, 59, 51, 0, 72, 53, 51, 37 };
int controls[9] = { 95, 71, 91, 49, 37, 71, 44, 75, 40 };
int controls_cmd[10] = { 25, 75, 51, 0, 83, 37, 37, 71, 26, 75 };
int rules[6] = { 81, 25, 44, 51, 75, 40 };
int rules3[10] = { 63, 44, 83, 87, 0, 26, 51, 44, 44, 118 };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The texture:
&lt;img src=&quot;/assets/texture_atlas.jpg&quot; alt=&quot;Texture atlas&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the check used for checking for the win condition and rendering the flag onto screen.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if (hoho == mhm) {
	startx = 0.03f;
	starty = .35;
	deltax = .029;
	deltay = .06;
	if (UV.x &amp;gt; startx &amp;amp;&amp;amp; UV.x &amp;lt; startx + 32. * deltax &amp;amp;&amp;amp; UV.y &amp;gt; starty &amp;amp;&amp;amp; UV.y &amp;lt; starty + deltay) {
		int i = int((UV.x - startx) / deltax);
		float fi = float(i);
		int q = int(d[i] ^ mhm2.x ^ mhm2.y);
		int qy = q / 11;
		q %= 11;
		float cx = tux * float(q) + (UV.x - (startx + deltax * float(fi))) * tux / deltax;
		float cy = tuy * float(qy) + ((UV.y - starty) * tuy / deltay);
		float r = texture(tex2, vec2(cx, cy)).r;
		color = mix(vec4(r) * vec4(sin(tock*.1 + uv.y), cos(tock*3 + uv.x), .0, 1.), color, 1. - r);
	}
	deltax = 0.025;
	deltay = 0.05;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The snippet computes the index in the text atlas with &lt;code class=&quot;highlighter-rouge&quot;&gt;int(d[i] ^ mhm2.x ^ mhm2.y&lt;/code&gt;` and then renders it.&lt;/p&gt;

&lt;p&gt;In fact we can edit the shaders in RenderDoc to see what happens.
Let’s change the if condition to always evaluate to true and modify the index computation to be &lt;code class=&quot;highlighter-rouge&quot;&gt;int((d[i] ^ mhm2.x ^ mhm2.y) &amp;amp; 0xFF)&lt;/code&gt; and then we get:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/broken_flag.jpg&quot; alt=&quot;Broken flag&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s some broken text but at least we learn that we should get the flag printed in the middle when we reach a win condition.
The win value is contained in the &lt;code class=&quot;highlighter-rouge&quot;&gt;mhm&lt;/code&gt; uniform and its value is set to &lt;strong&gt;1337&lt;/strong&gt;.
So, the player has to get a score of 1337 to get the flag.&lt;/p&gt;

&lt;p&gt;Furthermore, there are the two values &lt;code class=&quot;highlighter-rouge&quot;&gt;mhm2.x&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;mhm2.y&lt;/code&gt; with which the index is xor-ed.
Inspecting the values does not reveal anything but they seem to be some large xor masks, correspondingly &lt;strong&gt;0x71272A0A&lt;/strong&gt; and &lt;strong&gt;0x60ABD0BD&lt;/strong&gt;.
The two values will be relevant later.&lt;/p&gt;

&lt;p&gt;Also, the &lt;code class=&quot;highlighter-rouge&quot;&gt;d[]&lt;/code&gt; array being accesses is the same as the one to which it is written in the compute shader.
This reveals the whole way the flag is generated. Each time the player scores, the &lt;code class=&quot;highlighter-rouge&quot;&gt;d[]&lt;/code&gt; array gets updated. Once a score of 1337 is reached, the fragment shader computes the indices in the text atlas using the &lt;code class=&quot;highlighter-rouge&quot;&gt;d[]&lt;/code&gt; array and the two values in the &lt;code class=&quot;highlighter-rouge&quot;&gt;mhm2&lt;/code&gt; vector.&lt;/p&gt;

&lt;p&gt;We can try to get the flag by again modifying the shader to reach this condition.
First we remove the early &lt;code class=&quot;highlighter-rouge&quot;&gt;return&lt;/code&gt; in the compute shader even if the game is over.
Then, we rewrite the code for incrementing the score and decoding the flag to:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for (int i = 0; i &amp;lt; 1337; ++i) {
    if (row == 0)
    {
        if ((col != 0u) &amp;amp;&amp;amp; (col != 15u))
        {
            for (int r = int(row); r &amp;lt; 32; r++)
            {
                _234.arr2[(r * 16) + int(col)] = _35.arr[((r + 1) * 16) + int(col)];
                _35.arr[(r * 16) + int(col)] = _35.arr[((r + 1) * 16) + int(col)];
            }
        }
        if (col == 0u)
        {
            _299.abra++;
        }
    }
    memoryBarrierShared();
    barrier();
    if (row == 0)
    {
        uint q = _299.d[(col + uint(_299.abra)) % 32u];
        q ^= (uint(_299.abra) + col);
        _299.d[(col + uint(_299.abra)) % 32u] = (q &amp;lt;&amp;lt; uint(25)) | (q &amp;gt;&amp;gt; uint(10));
        q = _299.d[((col + uint(_299.abra)) + 16u) % 32u];
        q ^= ((uint(_299.abra) + col) + 16u);
        _299.d[((col + uint(_299.abra)) + 16u) % 32u] = (q &amp;lt;&amp;lt; uint(25)) | (q &amp;gt;&amp;gt; uint(10));
    }
    memoryBarrierShared();
    barrier();
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We save by clicking F5 and then re-run the captured frame by clicking Tools -&amp;gt; Start Replay Loop.
And then we get a broken flag again:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/broken_flag2.jpg&quot; alt=&quot;Broken flag 2&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It looks like there might be something else happening, so we have to examine the binary.&lt;/p&gt;

&lt;h3 id=&quot;assembly-debugger&quot;&gt;Assembly debugger&lt;/h3&gt;
&lt;p&gt;If we open the executable in pebear, then we can see that there is only one section named &lt;code class=&quot;highlighter-rouge&quot;&gt;peippo&lt;/code&gt; and that the only two imports are &lt;code class=&quot;highlighter-rouge&quot;&gt;LoadLibraryA&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;GetProcAddress&lt;/code&gt;. That’s not very helpful.&lt;/p&gt;

&lt;p&gt;By opening it in x32dbg we can examine it and observe that in fact the game is packed using an unknown custom packer.
The CFG is the following:
&lt;img src=&quot;/assets/packed_graph.jpg&quot; alt=&quot;CFG depacker&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we just set a breakpoint on the last jmp instruction and reach it, we can see that the graph was altered after unpacking:
&lt;img src=&quot;/assets/unpacked_graph.jpg&quot; alt=&quot;CFG unpacked&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I find it useful to step-through the execution of the program in the CFG.
Eventually we reach our first anti-debugger check which reads the PEB::BeingDebugged byte and sets the result at address 0x424FD4.
&lt;img src=&quot;/assets/first_anti_debug.jpg&quot; alt=&quot;Anti-debug PEB BeingDebugged&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can continue stepping-through and we reach another anti-debugger check which queries the running processes, computes a hash of their names and compares them against known hashes.
&lt;img src=&quot;/assets/second_anti_debug.jpg&quot; alt=&quot;Anti-debug process name hashes&quot; class=&quot;center-image&quot; /&gt;
This anti-debug check is bad since it can give false positives and also makes the game unwinnable if the player happens to have processes which happen to collide with the hashes.
but it’s easy to verify that some debuggers like OllyDbg, x32dbg and RenderDoc are detected via it in a sneaky way without making any strings present.
The check sets the value at 0x424488 to 1 if a debugger process is running.&lt;/p&gt;

&lt;p&gt;After setting “Hardware break points” on the addresses 0x424488 and 0x424FD4, we eventually reach the places where the values are used.
&lt;img src=&quot;/assets/update_byte_if_no_debugger.jpg&quot; alt=&quot;Set byte to 7&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/update_byte_if_no_process.jpg&quot; alt=&quot;Set byte to 25&quot; class=&quot;center-image&quot; /&gt;
One of the values is set if no debugger is attached and the other one if no debugger process is running.&lt;/p&gt;

&lt;p&gt;To find out the intention of setting these bytes only under these conditions we have to continue stepping through the program.
If we continue examining resources, we can see the text atlas being unpacked and loaded into a GL texture but that’s not where the bytes are being written.&lt;/p&gt;

&lt;p&gt;Let’s examine the shaders and shader binary which are used. Unfortunately, since the exported GL functions depend on context initialization, the driver does not statically export them and thus we cannot find it using x32dbg at least in the list of exported functions in the nvogl32.dll (NVIDIA user-space GL driver).
But we can search for the function strings which are passed to &lt;code class=&quot;highlighter-rouge&quot;&gt;wglGetProcAddress&lt;/code&gt; and set hardware break points.
Eventually we find out that the written bytes above are targetting the SPIR-V binary.&lt;/p&gt;

&lt;p&gt;We can run the program twice: once with debugger attached and running, and once with both values unset by us.
Then we can dump both SPIR-V binaries, decompile them using any of the Khronos tools and compare them:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;266c266
&amp;lt;         _299.d[(col + uint(_299.abra)) % 32u] = (q &amp;lt;&amp;lt; uint(13)) | (q &amp;gt;&amp;gt; uint(10));
---
&amp;gt;         _299.d[(col + uint(_299.abra)) % 32u] = (q &amp;lt;&amp;lt; uint(25)) | (q &amp;gt;&amp;gt; uint(7));
269c269
&amp;lt;         _299.d[((col + uint(_299.abra)) + 16u) % 32u] = (q &amp;lt;&amp;lt; uint(13)) | (q &amp;gt;&amp;gt; uint(10));
---
&amp;gt;         _299.d[((col + uint(_299.abra)) + 16u) % 32u] = (q &amp;lt;&amp;lt; uint(25)) | (q &amp;gt;&amp;gt; uint(7));
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;So, so apparently we were working with the unmodified shader in RenderDoc.&lt;/p&gt;

&lt;p&gt;Furthermore, the contents of the fragment shader and the spirv binary are used to compute the two hash values with which the text atlas indices are xor-ed when displaying the flag:
&lt;img src=&quot;/assets/compute_hash.jpg&quot; alt=&quot;Compute hash&quot; class=&quot;center-image&quot; /&gt;
The two values with the anti-debugger checks disabled are: 0x71272A0A and 0x515ECC80.&lt;/p&gt;

&lt;p&gt;We can continue investigating and notice that we enter the game loop and that there are no more sneaky checks any longer.
With this we can move to the actual solution.&lt;/p&gt;

&lt;h3 id=&quot;dynamic-solution-in-renderdoc&quot;&gt;Dynamic solution in RenderDoc&lt;/h3&gt;
&lt;p&gt;A dynamic solution in RenderDoc would be easy since the debugger provides us with an easy interface for editing shaders and reloading them from a fixed state.&lt;/p&gt;

&lt;p&gt;First we modify the compute shader to increase the score 1337 times and to apply each iteration of the rotation hash:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for (int i = 0; i &amp;lt; 1337; ++i) {
	if (row == 0)
	{
		if ((col != 0u) &amp;amp;&amp;amp; (col != 15u))
		{
			for (int r = int(row); r &amp;lt; 32; r++)
			{
				_234.arr2[(r * 16) + int(col)] = _35.arr[((r + 1) * 16) + int(col)];
				_35.arr[(r * 16) + int(col)] = _35.arr[((r + 1) * 16) + int(col)];
			}
		}
		if (col == 0u)
		{
			_299.abra++;
		}
	}
	memoryBarrierShared();
	barrier();
	if (row == 0)
	{
		uint q = _299.d[(col + uint(_299.abra)) % 32u];
		q ^= (uint(_299.abra) + col);
		_299.d[(col + uint(_299.abra)) % 32u] = (q &amp;lt;&amp;lt; uint(25)) | (q &amp;gt;&amp;gt; uint(7));
		q = _299.d[((col + uint(_299.abra)) + 16u) % 32u];
		q ^= ((uint(_299.abra) + col) + 16u);
		_299.d[((col + uint(_299.abra)) + 16u) % 32u] = (q &amp;lt;&amp;lt; uint(25)) | (q &amp;gt;&amp;gt; uint(7));
	}
	memoryBarrierShared();
	barrier();
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next we edit the fragment shader to use the correct shader hashes:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if (hoho == mhm) {
	startx = 0.03f;
	starty = .35;
	deltax = .029;
	deltay = .06;
	if (UV.x &amp;gt; startx &amp;amp;&amp;amp; UV.x &amp;lt; startx + 32. * deltax &amp;amp;&amp;amp; UV.y &amp;gt; starty &amp;amp;&amp;amp; UV.y &amp;lt; starty + deltay) {
		int i = int((UV.x - startx) / deltax);
		float fi = float(i);
		int q = int(d[i] ^ 0x71272A0A ^ 0x515ECC80);
		int qy = q / 11;
		q %= 11;
		float cx = tux * float(q) + (UV.x - (startx + deltax * float(fi))) * tux / deltax;
		float cy = tuy * float(qy) + ((UV.y - starty) * tuy / deltay);
		float r = texture(tex2, vec2(cx, cy)).r;
		color = mix(vec4(r), color, 1. - r);
	}
	deltax = 0.025;
	deltay = 0.05;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Remember to click the Refresh button in RenderDoc for both shaders, then go to Tools -&amp;gt; Start replay loop and there you go:
&lt;img src=&quot;/assets/dynamic_flag.jpg&quot; alt=&quot;The real flag&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;a-static-solution&quot;&gt;A static solution&lt;/h3&gt;
&lt;p&gt;Solving the challenge is also possible without a graphics debugger but it requires one to extract the text atlas, shaders and spirv binary. All of that can be done with the earlier steps but it might be a bit more difficult to analyze and experiment with.&lt;/p&gt;

&lt;p&gt;Decompiling the SPIR-V binary can be done with SPIRV-Cross which outputs a GLSL shader.
In fact RenderDoc uses that tool internally to decompile the SPIR-V binary.&lt;/p&gt;

&lt;p&gt;After extracting all of the shaders and GPU buffers, one can write a decoder:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;h1=0x71272a0a
h2=0x515ecc80
bt = [
0x34EDC49F, 0x16ACC9CF, 0x705071FF, 0x5370F9EF,
0x14BAC19F, 0x36F8C9DB, 0x50357BEB, 0x726F73FB,
0x85348B0A, 0xE746831F, 0x81BA198F, 0xA3C8419F,
0x4C8FF9EF, 0x3ACD71FF, 0x5C3D69E5, 0x7E48F4F5,
0xA7FB8D06, 0x80CB0516, 0x46331D24, 0x64769464,
0x8BA38414, 0xA9958C04, 0xE5499434, 0xC7309C31,
0x3AE76E40, 0x18BF2650, 0x7CD73E60, 0x16BF3671,
0xF9678EA5, 0xDB24D4B5, 0xBDD94C85, 0x8B01C495]

target=1337
for u in range(target):
    for v in range(32):
        q = (u+1+v) ^ bt[(u+1+v)%32]
        bt[(u+1+v)%32] = ((q&amp;lt;&amp;lt;25)&amp;amp;0xFFFFFFFF) | (q&amp;gt;&amp;gt;7)
for u in range(32):
    bt[u] = bt[u] ^ h1 ^ h2

text_map = {}
text_map[(6,3)] = &quot;h&quot;
text_map[(7,1)] = &quot;x&quot;
text_map[(8,3)] = &quot;n&quot;
text_map[(5,10)] = &quot;{&quot;
text_map[(9,7)] = &quot;3&quot;
text_map[(9,4)] = &quot;0&quot;
text_map[(3,8)] = &quot;p&quot;
text_map[(10,2)] = &quot;$&quot;
text_map[(4,4)] = &quot;_&quot;
text_map[(8,5)] = &quot;G&quot;
text_map[(5,8)] = &quot;P&quot;
text_map[(7,3)] = &quot;U&quot;
text_map[(5,5)] = &quot;F&quot;
text_map[(6,5)] = &quot;o&quot;
text_map[(3,4)] = &quot;r&quot;
text_map[(8,0)] = &quot;7&quot;
text_map[(4,7)] = &quot;e&quot;
text_map[(4,5)] = &quot;t&quot;
text_map[(9,5)] = &quot;1&quot;
text_map[(6,9)] = &quot;s&quot;
text_map[(9,6)] = &quot;2&quot;
text_map[(8,2)] = &quot;9&quot;
text_map[(8,10)] = &quot;}&quot;

flag = &quot;&quot;
for u in range(32):
    row = int(bt[u] / 11)
    col = bt[u] % 11
    if (row, col) not in text_map:
        print(&quot;{},{} not found in text_map&quot;.format(row,col))
        flag = flag + &quot;?&quot;
    else:
        flag += text_map[(row, col)]
print(flag)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the &lt;code class=&quot;highlighter-rouge&quot;&gt;text_map&lt;/code&gt; dictionary has entries only for what’s queried from the text atlas.&lt;/p&gt;

&lt;p&gt;The script prints out the flag:
&lt;code class=&quot;highlighter-rouge&quot;&gt;hxp{300$_GPU_For_7etr1s_1n_2019}&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-story-behind-the-challenge&quot;&gt;The story behind the challenge&lt;/h3&gt;
&lt;p&gt;At some point during the CTF preparation there was a shortage of RE challenges, so after a bump into &lt;code class=&quot;highlighter-rouge&quot;&gt;kirschju&lt;/code&gt; we agreed that I can contribute with something small.
The challenge was really supposed to be a filler for the RE category and be developed using whatever tools I had left from my demoscene days.&lt;/p&gt;

&lt;p&gt;The initial version was built was on top of the &lt;a href=&quot;http://www.lofibucket.com/articles/64k_intro.html&quot;&gt;Macau Exports 64k tool&lt;/a&gt; and used &lt;a href=&quot;https://martinradev.github.io/jekyll/update/2019/05/29/writing-a-pe32-x86-exe-packer.html&quot;&gt;my packer&lt;/a&gt; to get the binary small and also thwart static analysis and patching without at least running it.
Eventually I switched to &lt;a href=&quot;https://github.com/msqrt/glsl-testbench&quot;&gt;another framework&lt;/a&gt; for creating the challenge since the original 64k tool was going at great lengths to keep the binary small and the code base was spaghetti.&lt;/p&gt;

&lt;p&gt;We ran into multiple issues during the development phase:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For some reason the game does not run on AMD when the SPIR-V binary is used instead of plain GLSL.&lt;/li&gt;
  &lt;li&gt;For some reasons GLSL’s atomicMin does not work on Intel and the Intel compiler does not complain. Maybe it’s an issue coming from the SPIR-V translation.
The fix was to rewrite the code to use a min-reduction.&lt;/li&gt;
  &lt;li&gt;The NVIDIA user-space driver used to corrupt its heap once glSpecializeShader was called. That could be a bug coming from me.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope the challenge was enjoyable and did not cause much frustration for those who were not able to run it.&lt;/p&gt;

</description>
        <pubDate>Sun, 29 Dec 2019 00:00:00 +0100</pubDate>
        <link>http://yourdomain.com/jekyll/update/2019/12/29/tetris-re.html</link>
        <guid isPermaLink="true">http://yourdomain.com/jekyll/update/2019/12/29/tetris-re.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>Writing an executable packer for Win32</title>
        <description>&lt;h3 id=&quot;a-very-short-introduction-to-64k-demos&quot;&gt;A very short introduction to 64k demos&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Demoscene&quot;&gt;demoscene&lt;/a&gt; features various demo competitions for modern systems. The most notable categories are &lt;strong&gt;PC demo&lt;/strong&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/64K_intro&quot;&gt;64k intro&lt;/a&gt; and &lt;strong&gt;4k intro&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For me, the followng two reasons have made me prefer 64k demos:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;it opens opportunities to implement a complex lighting solution, include interesting geometry, have multiple layers of post-processing.&lt;/li&gt;
  &lt;li&gt;it keeps professional tools out for the most part, thus leaving technical problems for the group to solve.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-goes-into-a-64kb-exe&quot;&gt;What goes into a 64kb exe?&lt;/h3&gt;
&lt;p&gt;The minimum the program has to do is open a window, create a rendering context and setup a graphics pipeline. Depending on the rendering choices it might happen that geometry and textures are generated on the CPU and that there are many and complex rendering paths. This contributes to a fairly big code section.&lt;/p&gt;

&lt;p&gt;More and more of the work has been pushed onto the GPU which is programmed through shaders. They are often stored in plain GLSL source code or D3D shader bytecode.&lt;/p&gt;

&lt;p&gt;A music synthesizer of some sort and note data which will contribute to both the code and data sections.&lt;/p&gt;

&lt;p&gt;The rest is an overhead coming from the used executable format and the amount of imported functions.&lt;/p&gt;

&lt;p&gt;Considering that a “Hello world” program is already close or above this limit, one can begin to worry in one’s success of developing a 64k intro.
It turns out, it can be &lt;a href=&quot;http://www.lofibucket.com/articles/64k_intro.html&quot;&gt;challenging but possible with some care and creativity&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If we could somehow &lt;strong&gt;create a 200kb executable and compress it into a 64kb executable&lt;/strong&gt;, we would likely have &lt;strong&gt;more to show than an ordinary 64kb executable.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here’s a distribution of the three biggest sections in the uncompressed executable for our old intro &lt;a href=&quot;https://www.youtube.com/watch?v=yei3mJm33SQ&quot;&gt;Guberniya&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/packer_data/sect_distr.jpg&quot; alt=&quot;Guberniya sections&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, it’s around 278 kb, most of which is data but not executable code.&lt;/p&gt;

&lt;p&gt;To be able to enter the 64K category we would have to compress the executable in some way.&lt;/p&gt;

&lt;p&gt;This is where a thing called an exe-packer comes into play.&lt;/p&gt;

&lt;h3 id=&quot;what-is-an-exe-packer&quot;&gt;What is an exe-packer?&lt;/h3&gt;
&lt;p&gt;An &lt;a href=&quot;https://en.wikipedia.org/wiki/Executable_compression&quot;&gt;executable packer&lt;/a&gt; is a program which takes an &lt;strong&gt;Input program&lt;/strong&gt; and outputs an &lt;strong&gt;Output program&lt;/strong&gt; hopefully a lot smaller than the original one.
The output program is still a binary executable and no unpacking to disk is necessary. The reduction of size is achieved through compression.&lt;/p&gt;

&lt;p&gt;The machine code and data of the original executable are compressed through the use of a suitable compression algorithm.
A small decompression stub becomes the entry point of the new program which will take the compressed data, decompress it somewhere in virtual memory, make some adjustments and jump into it to
start the execution of the original program.&lt;/p&gt;

&lt;p&gt;Many of the size-limited demos make use of &lt;a href=&quot;http://www.farbrausch.de/~fg/kkrunchy/&quot;&gt;.kkrunchy&lt;/a&gt; and &lt;a href=&quot;https://in4k.github.io/wiki/crinkler&quot;&gt;Crinkler&lt;/a&gt;, correspondingly suited for 64KB and 4KB intros.&lt;/p&gt;

&lt;p&gt;Almost all demo groups use them and both offer excellent compression rates and small decompression stubs.&lt;/p&gt;

&lt;h3 id=&quot;why-write-your-own-packer&quot;&gt;Why write your own packer?&lt;/h3&gt;
&lt;p&gt;It’s a cross of software engineering, low-level programming and science.&lt;/p&gt;

&lt;p&gt;Working on the project wasn’t about improving over other solutions but rather about learning and improving my skills.
It helped me learn a lot more about data compression, how a PE32 binary looks like and a little bit about x86 assembly and its encoding.&lt;/p&gt;

&lt;h3 id=&quot;why-32-bit&quot;&gt;Why 32-bit?&lt;/h3&gt;
&lt;p&gt;I preferred keeping some comfort when working on the project. Typically, a 64-bit executable is bigger than a 32-bit one, thus some extra effort might have been necessary into making it all fit.
Compiled code for x86-64 will likely often include RIP-relative addressing which means that special care might be necessary to preprocess the data, so that it compresses better.
Also, our existing engine had been developed in the mindset that it would be running as a 32-bit exe, so there were likely some incompatibilities (or bugs) which we did not want to resolve.&lt;/p&gt;

&lt;h3 id=&quot;the-compressor&quot;&gt;The compressor&lt;/h3&gt;

&lt;p&gt;Before going into details of each step, let me give you an overview of the pipeline.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First we apply our domain knowledge to preprocess the data, so that it compresses better.&lt;/li&gt;
  &lt;li&gt;We use LZ77 to compress the original data.&lt;/li&gt;
  &lt;li&gt;We use a very simple adaptive 0-order model and arithmetic coder for the final pass.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;lz77&quot;&gt;LZ77&lt;/h4&gt;

&lt;p&gt;I stopped at LZ77 for its simplicity and good results. However, there are multiple other benefits I did not consider at the beginning.&lt;/p&gt;

&lt;p&gt;First, it is an asymmetric compressor. The compressor might run slow since it has to find a good way to parse and encode the input.
However, the decompressor is blazingly fast since it boils down to only memcopies and very few branches.&lt;/p&gt;

&lt;p&gt;Second, in the &lt;a href=&quot;http://www-math.mit.edu/~shor/PAM/lempel_ziv_notes.pdf&quot;&gt;limit of the string S with length N, the average bits/byte reaches the entropy of the source H(S)/N&lt;/a&gt;. Thus, with LZ77 we can still reach the absolute best we can do in terms of compression rate. The only issue is, of course, how fast we reach the limit.&lt;/p&gt;

&lt;p&gt;LZ77 is really simple and the &lt;a href=&quot;https://en.wikipedia.org/wiki/LZ77_and_LZ78#LZ77&quot;&gt;wikipedia page explains it really well&lt;/a&gt;. I will try to focus on the details of my implementation which helped me reach better results over time:&lt;/p&gt;

&lt;p&gt;The three important parameters are the &lt;strong&gt;length&lt;/strong&gt; and &lt;strong&gt;distance&lt;/strong&gt; of references, and the minimum &lt;strong&gt;repetition length&lt;/strong&gt; for which we would use a reference instead of directly copying the bytes into the output stream.&lt;/p&gt;

&lt;p&gt;If the &lt;strong&gt;distance&lt;/strong&gt; is too small, then we will not be able to use repetitions much earlier in the input. If too long, then we might be wasting bits in representing the distance.&lt;/p&gt;

&lt;p&gt;If the &lt;strong&gt;length&lt;/strong&gt; is too small, then we will not be able to handle very long repetitions with a single reference. If too big, again we will be wasting bits.&lt;/p&gt;

&lt;p&gt;If the minimum repetition length is too small, then we will be wasting bits on encoding very short references.
If too long, we will be directly copying data which could have been encoded through references.&lt;/p&gt;

&lt;p&gt;I ended up selecting the following configuration:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Distance: 16 bits&lt;/strong&gt;. We would like to be able to refer to data much earlier in the input.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Length: 8 bits&lt;/strong&gt;. We do not expect to have very long repetitions. The length of a reference is thus at most 255.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Minimum repetition length&lt;/strong&gt;: 3 bytes (24 bits). Repetitions of length 3 or less can directly be copied.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Type: 1 bit&lt;/strong&gt;. We need it to disambiguate between references and direct copies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/packer_data/lz77.jpg&quot; alt=&quot;LZ77&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can do a small optimization with respect to reference lengths.&lt;/p&gt;

&lt;p&gt;If we are creating a reference, then we know that the repetition was bigger than 3. We can subtract (3+1) from the length of the repetition and use that instead. This adds the possibility to handle repetitions of 259 instead of the initial 255, thus saving some bytes potentially. This improved compression slightly.&lt;/p&gt;

&lt;h4 id=&quot;an-optimal-parser-assuming-no-entropy-coding&quot;&gt;An optimal parser assuming no entropy coding&lt;/h4&gt;
&lt;p&gt;We will be doing entropy coding afterwards, but let’s assume that we won’t. Thus the cost of a reference and direct copy will both be 25 bits.&lt;/p&gt;

&lt;p&gt;How do we figure out when to use a reference and when to directly copy data, so that the total cost at the end is minimum?&lt;/p&gt;

&lt;p&gt;We can try to design an algorithm linear in the input length using &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_programming&quot;&gt;dynamic programming&lt;/a&gt;.
Let’s have an array &lt;strong&gt;best[]&lt;/strong&gt; for which &lt;strong&gt;best[k]&lt;/strong&gt; gives us the minimum cost for which we can parse the input &lt;strong&gt;from beginning till position k&lt;/strong&gt;.
Suppose we have somehow computed the minimum costs for each position up to k. How can we update the value for position &lt;strong&gt;k+1&lt;/strong&gt;?
Well, we can reach position &lt;strong&gt;k+1&lt;/strong&gt; in multiple different ways:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Through a direct copy. Thus, from an earlier position we have copied 3 bytes and we have reached position k+1. This means that the cost would be &lt;strong&gt;best[k+1] = best[k-2] + 24 + 1 bits&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Through a reference where the length could go from 4 to 259. Thus the cost would be &lt;strong&gt;best[k+1] = min(best[k-3], best[k-4], …, best[k-258]) + 24 + 1 bits&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since we can take either, the final cost becomes:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;best[k+1] = min(best[k-2], best[k-3], …, best[k-258]) + 24 + 1 bits&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At the beginning we set best[0] = 0 and best[1…N] to infinity. We process the array from left to right keeping track of the selection which gave us the best result.&lt;/p&gt;

&lt;p&gt;Once we are done processing the array, we can go from end to beginning to recover whether a reference or a direct copy should be made.&lt;/p&gt;

&lt;h4 id=&quot;filtering-the-code-to-improve-compression&quot;&gt;Filtering the code to improve compression&lt;/h4&gt;

&lt;p&gt;When addresses of functions are known during compile/link time, relative calls are used - one because it allows the binary to be a PIE, and two because absolute calls would require having the address be first stored into a register or read from memory in the form of an indirect absolute call.
Thus, the majority of calls in our executable are relative calls. I measured that around &lt;strong&gt;10% of the bytes in .text section are for relative calls&lt;/strong&gt; - 1 byte op-code and 4 bytes offset.&lt;/p&gt;

&lt;p&gt;Because the calls are relative, the 4-byte offset will vary since the call is always done from a different position in memory. We can transform the relative-offset into an absolute address by adding the address of the opcode.
This guarantees that for every place where a given function is called, the op-code and now-transformed offset will match. That is a total of 5 bytes which will possibly repeat multiple times.
The technique is known as the &lt;a href=&quot;http://mattmahoney.net/dc/dce.html#Section_571&quot;&gt;E8-E9 filter&lt;/a&gt; and more details can be found in this &lt;a href=&quot;https://fgiesen.wordpress.com/2011/01/24/x86-code-compression-in-kkrunchy/&quot;&gt;post by ryg&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Applying the E8-E9 filter yielded around 10% improvement in compression ratio which is huge given its simplicity.&lt;/p&gt;

&lt;p&gt;Later the filter was replaced with &lt;a href=&quot;http://www.farbrausch.de/~fg/code/disfilter/&quot;&gt;.kkrynchy’s disfilter&lt;/a&gt;. There was an improvement of around 6% over the E8-E9 filter.&lt;/p&gt;

&lt;h4 id=&quot;arithmetic-coding&quot;&gt;Arithmetic coding&lt;/h4&gt;

&lt;p&gt;There wasn’t anything special with regards how the arithmetic coder was implemented. I just followed the slides from a &lt;a href=&quot;https://courses.helsinki.fi/sites/default/files/course-material/4524672/DCT-lecture03.pdf&quot;&gt;course I took in my B.S&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;designing-a-model&quot;&gt;Designing a model&lt;/h4&gt;

&lt;p&gt;First some definitions:&lt;/p&gt;

&lt;p&gt;The arithmetic coder needs to know the &lt;strong&gt;cumulative distribution&lt;/strong&gt; of the symbols it has to encode/decode. For that we have to design a model which gives us a distribution which hopefully matches the actual distribution of the data we want to compress.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;static model&lt;/strong&gt; is a model for which the probability of each symbol is pre-computed and stored. We cannot have a static model at all since the final binary has to be very small.&lt;/p&gt;

&lt;p&gt;An &lt;strong&gt;adaptive model&lt;/strong&gt; maintains the distribution of only the last K symbols. That distribution is likely closer to the distribution of the random variable which governs the next symbol in the stream.&lt;/p&gt;

&lt;p&gt;I stopped at a &lt;a href=&quot;https://www.cs.helsinki.fi/u/tpkarkka/opetus/12k/dct/lecture05.pdf&quot;&gt;0-order&lt;/a&gt; adaptive model. A 0-order context would mean the model does not consider what any of the previous characters are to estimate what the probability of next symbol should be.&lt;/p&gt;

&lt;p&gt;There were some complications in designing a model:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;When having an adaptive model, we run into the &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=2263404.2271157&quot;&gt;zero frequency problem&lt;/a&gt;. The solution, although bad, was to set the initial frequency of each symbol to 1. This means that we are paying a cost for symbols which might never occur.&lt;/li&gt;
  &lt;li&gt;As mentioned before, the reference-distance is 16 bits. The distribution will be very sparse but we will still be paying due to the zero frequency problem.
Moreover, we want to be able to update the distribution fast by either decreasing or increasing the frequency of the symbol. And we need the cumulative probability of a symbol. Thus, if we update the frequency of a symbol at the beginning (and this will happen often since the reference distance will often be small), then we have to update all the subsequent ~2^16 elements. A possibility is to use a &lt;a href=&quot;https://en.wikipedia.org/wiki/Fenwick_tree&quot;&gt;Fenwick tree&lt;/a&gt; but the implementation will be slightly bigger thus making the decompresor bigger.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The solution was to split the 16-bit distance into two values - &lt;strong&gt;distance_low&lt;/strong&gt; and &lt;strong&gt;distance_high&lt;/strong&gt;. Each 8 bits.
This means two things:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The distributions of distance_low and distance_high will be less sparse and thus there would be less waste due to the 1 frequency initialization.&lt;/li&gt;
  &lt;li&gt;Updates and queries can be done naively but still be fast enough.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;optimal-parsing&quot;&gt;“Optimal” parsing&lt;/h4&gt;

&lt;p&gt;Yet another complication. Remember that optimal parser from before?&lt;/p&gt;

&lt;p&gt;It’s not optimal with respect to the output of the entropy coder.
Why?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;References will now likely no longer have a uniform cost of 25 bits. Some will be cheaper, some more costly. This would be the case with a static model as well.&lt;/li&gt;
  &lt;li&gt;Copied bytes will also no longer cost 25 bits. This would be true also for a static model.&lt;/li&gt;
  &lt;li&gt;We have an adaptive model, thus a choice at position &lt;strong&gt;i&lt;/strong&gt; will change the distribution also effecting future choices.&lt;/li&gt;
  &lt;li&gt;Even if we know how to get the lowest cost up to a position i, there could be &lt;strong&gt;exponentially many different paths&lt;/strong&gt; to reach that position with that cost. Every path could define a different distribution of symbols. The parser would have to use the formed distributions up to this point to know the best way to reach i+1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, we see it’s an issue which affects compression rate. We also see that it’s a computationally difficult problem. You can read &lt;a href=&quot;http://cbloomrants.blogspot.com/2008/10/10-10-08-7_10.html&quot;&gt;this article from Cbloom for more info&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, how do we compute the cost?&lt;/p&gt;

&lt;p&gt;The entropy coder will output &lt;strong&gt;l&lt;/strong&gt; number of bits per symbol where &lt;strong&gt;l&lt;/strong&gt; depends on the probability &lt;strong&gt;p&lt;/strong&gt; of that symbol.
It turns out that the &lt;a href=&quot;https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem&quot;&gt;length &lt;strong&gt;l&lt;/strong&gt; which minizes the average code word length is: &lt;strong&gt;l = -log2(p)&lt;/strong&gt;&lt;/a&gt;. The arithmetic coder will encode each symbol with length very close to the optimal one.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The cost of a reference becomes: &lt;strong&gt;cost = -(log2(prob_distance_low) + log2(prob_distance_high) + log2(prob_length) + log2(prob_type_reference))&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;The cost of a direct copy becomes: &lt;strong&gt;cost = -(log2(prob_symb_0) + log2(prob_symb_1) + log2(prob_symb_2) + log2(prob_type_copy))&lt;/strong&gt; where we also have to update the distribution after each of the symbols since we are using an adaptive model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since it’s not clear which path is the best one and should be used, we can keep track of the &lt;strong&gt;first K-paths with the smallest cost to reach a given position&lt;/strong&gt;.
So, we have the possibly best K-paths for each index from 0 to R, then to compute the best K-paths for R+1 we can do the following:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def computeBest(i):
{
    struct Selection
    {
        cost = INF
        distribution = Empty
    }
    K_MAX = 256 // Keep at most 256 &#39;best&#39; paths.
    bestSelection[i] = MinHeap(K_MAX) 
    // First try a reference
    for (j = 4; j &amp;lt; WindowMaxLength &amp;amp;&amp;amp; j &amp;lt;= i; ++j)
    {
        if canReach(from = i-j, to = i)
        {
            cost, distribution = computeRefCost(bestSelection[i-jj], i)
            bestSelection[i].insert(cost, distribution)
        }
    }
    // Now try to copy bytes
    cost, distribution = computeCopyCost(bestSelection[i-3])
    bestSelection[i].insert(cost, distribution)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This improved the compression rate significantly!&lt;/p&gt;

&lt;h3 id=&quot;finishing-the-packer&quot;&gt;Finishing the packer&lt;/h3&gt;
&lt;p&gt;As we have to generate our own executable, some knowledge of the PE32 format is necessary. The following two resources were enough to complete the packer.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://bytepointer.com/resources/pietrek_peering_inside_pe.htm&quot;&gt;Peering Inside the PE: A Tour of the Win32 Portable Executable File Format, Matt Pietrek&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://sandsprite.com/CodeStuff/Understanding_imports.html&quot;&gt;Understanding the Import Address Table&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;duties-of-the-packer-and-depacker&quot;&gt;Duties of the packer and depacker&lt;/h4&gt;
&lt;p&gt;Using our compressor we can compress all of the &lt;strong&gt;sections of the original exe&lt;/strong&gt;.
Our decompressor stub will be the first thing executed in the new program.
In order to guarantee that the new program produces the same output as the original, the depacker has the following duties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It should decompress the sections into the correct places in memory.
We can figure out the correct addresses from the &lt;strong&gt;ImageBase&lt;/strong&gt; field and each section’s relative &lt;strong&gt;VirtualAddress&lt;/strong&gt; field.&lt;/li&gt;
  &lt;li&gt;It should handle imports. Since all of the sections of the original exe are compressed, the program loader would not know which are the imported libraries and functions.
It becomes our responsibility to populate the &lt;strong&gt;Import Address Table&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;It should undo the code filter.&lt;/li&gt;
  &lt;li&gt;Jump into memory where the original exe’s entry point is.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;a-one-section-exe&quot;&gt;A one-section exe&lt;/h4&gt;
&lt;p&gt;The new program will have exactly one big section. This potentially saves space as there are requirements on the alignment of each section in the file - 512 bytes. We also do not need more than one.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/packer_data/mem.jpg&quot; alt=&quot;One big section&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since this one section serves for data and code, we will require that it’s mapped with &lt;strong&gt;MEM_EXECUTE, MEM_READ and MEM_WRITE characteristic flags&lt;/strong&gt; set.
Since there are &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/desktop/api/winnt/ns-winnt-_image_section_header&quot;&gt;more possible characteristic flags&lt;/a&gt; and I did not know what I was doing, I decided that it’s a safe bet to go over all sections and &lt;strong&gt;OR (|) the flags&lt;/strong&gt;.
This worked but it might not have been necessary or erroneous.&lt;/p&gt;

&lt;p&gt;Let’s say that all of the depacker is 0x1000 bytes long, that the uncompressed sections take 0xA1230 bytes, and that original ImageBase is 0x400000 (the default one).&lt;/p&gt;

&lt;p&gt;Then we can have our new ImageBase as (0x400000-0x1000-0xA1230) &amp;amp; ~(PAGE_SIZE-1). We zero-out the last couple of bits due to page-alignment requirements.
0x1000 is reserved for the depacker stub. We are left with 0xA1230 bytes for scratch memory to decompress all of the sections. We can subtract more if we need more scratch memory.&lt;/p&gt;

&lt;h4 id=&quot;handling-imports&quot;&gt;Handling imports&lt;/h4&gt;
&lt;p&gt;As mentioned we have to handle imports. To do so, we can import a single library - &lt;strong&gt;KERNEL32.DLL&lt;/strong&gt; - and two functions - &lt;strong&gt;LoadLibraryA&lt;/strong&gt; and &lt;strong&gt;GetProcAddress&lt;/strong&gt;.
Our loader stub to handle imports can go over the Import Address Table (IAT), load the necessary libraries with LoadLibraryA and then call GetProcAddress to find the address of the functions necessary to import.
Then we can update each IAT entry with the virtual address of the corresponding function.&lt;/p&gt;

&lt;h4 id=&quot;removing-the-dos-stub&quot;&gt;Removing the DOS stub&lt;/h4&gt;
&lt;p&gt;Probably every PE32 article mentions that for legacy reasons many PE32 exes begin with a &lt;a href=&quot;https://wiki.osdev.org/PE#DOS_Stub&quot;&gt;DOS header and executable code&lt;/a&gt;.
We have to keep the header, at least most of it. We can remove the executable code as it will only be executed under DOS.
This saves some bytes.&lt;/p&gt;

&lt;h4 id=&quot;better-utilization-of-padding&quot;&gt;Better utilization of padding&lt;/h4&gt;
&lt;p&gt;Each section needs to be 512 bytes aligned. This means that there is likely empty space added for alignment before the section. We can take advantage of that to store additional information.
There was around 192 bytes of unused space, so an optimization was to copy the last 192 bytes of the compressed data before the start of the section.
Then in our depacker we can include a couple of instructions (e.g. &lt;strong&gt;rep movsb&lt;/strong&gt;) to append the “hidden bytes” to the back of the compressed data.&lt;/p&gt;

&lt;h4 id=&quot;handling-machine-code&quot;&gt;Handling machine code&lt;/h4&gt;
&lt;p&gt;We have to include into our new executable the pieces of machine code for copying data, decompressing data and handling imports.&lt;/p&gt;

&lt;p&gt;I wrote everything in C and used &lt;code class=&quot;highlighter-rouge&quot;&gt;__declspec(noinline)&lt;/code&gt; to prevent the compiler from inlining the functions of my interest.
I compiled using &lt;strong&gt;/Os&lt;/strong&gt; to ask the compiler to produce smaller code if possible.&lt;/p&gt;

&lt;p&gt;Then I used the Visual Studio debugger to get the machine code and instructions, copied them into a file and used a python script to generate a C byte-array which contains the machine code.
Then I would copy the C array of bytes into my packer.&lt;/p&gt;

&lt;p&gt;It’s quite a cumbersome process. It would have been possible to use DUMPBIN and have a python script which does all of this automatically.&lt;/p&gt;

&lt;p&gt;However, I did not have to go through the manual steps many times, so it wasn’t that bad.&lt;/p&gt;

&lt;h3 id=&quot;final-result&quot;&gt;Final result&lt;/h3&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/bdb8G_DedpY&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;You can also download the exe from the &lt;a href=&quot;http://www.pouet.net/prod.php?which=77392&quot;&gt;download link in pouet&lt;/a&gt;. Note that it doesn’t work on AMD.&lt;/p&gt;

&lt;p&gt;The uncompressed exe has a size of 165 KB.&lt;/p&gt;

&lt;p&gt;The compressed exe has a size of 62.1 KB which also includes the depacker stub.&lt;/p&gt;

&lt;p&gt;I recall that was slightly worse than 7-zip’s LZMA2 on maximum settings.&lt;/p&gt;

&lt;p&gt;The demo was made with &lt;a href=&quot;http://www.lofibucket.com/&quot;&gt;cce&lt;/a&gt; and &lt;a href=&quot;https://www.pouet.net/user.php?who=53077&quot;&gt;msqrt&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;releasing-the-code-and-packer&quot;&gt;Releasing the code and packer&lt;/h3&gt;

&lt;p&gt;I’m not releasing the packer. I’m not releasing the source code.&lt;/p&gt;

&lt;p&gt;Here are the reasons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The packer is not on par with .kkrunchy or &lt;a href=&quot;https://plot.ly/~yupferris/2//#/&quot;&gt;Ferris’ squishy&lt;/a&gt;, so there’s no use for it.&lt;/li&gt;
  &lt;li&gt;The packer doesn’t handle TLS and likely has bugs.&lt;/li&gt;
  &lt;li&gt;The source code is a mess.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But if you have questions, write me an email :)&lt;/p&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Thanks to &lt;a href=&quot;https://scholar.google.co.uk/citations?user=L5boL7MAAAAJ&amp;amp;hl=da&quot;&gt;Dmitry Kosolobov&lt;/a&gt; for answering my questions in our coincidental bumps on the busses in Helsinki.&lt;/p&gt;

&lt;p&gt;And thanks to &lt;a href=&quot;https://www.cs.helsinki.fi/u/tpkarkka/&quot;&gt;Juha Kärkkäinen&lt;/a&gt; for the excellent course on data compression.&lt;/p&gt;

</description>
        <pubDate>Wed, 29 May 2019 00:00:00 +0200</pubDate>
        <link>http://yourdomain.com/jekyll/update/2019/05/29/writing-a-pe32-x86-exe-packer.html</link>
        <guid isPermaLink="true">http://yourdomain.com/jekyll/update/2019/05/29/writing-a-pe32-x86-exe-packer.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
  </channel>
</rss>
